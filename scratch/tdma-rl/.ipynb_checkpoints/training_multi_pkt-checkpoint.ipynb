{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from ns3gym import ns3env\n",
    "from DQN_model import DeepQNetwork\n",
    "from DQN_model import Eval_Model\n",
    "from DQN_model import Target_Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1) # use GPU 1\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# #config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\"1\") # use GPU 1\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function learning rate\n",
    "learning_rate = 0.001\n",
    "# discount factor\n",
    "reward_decay = 0.9\n",
    "# 10% random select\n",
    "e_greedy = 0.9\n",
    "# copy weight from eval_net to target_net each N iter\n",
    "replace_target_iter = 100\n",
    "# size of experience pool\n",
    "memory_size = 5000\n",
    "batch_size = 32\n",
    "training_episodes = 20\n",
    "# MAXSLOTS\n",
    "MAXSLOTS = 3\n",
    "\n",
    "episode_time = 600\n",
    "# model path (Need to change)\n",
    "eval_model_weights_path = ''#'/workspace/20pkt_traffic_model/eval_model_weights'\n",
    "target_model_weights_path = ''#'/workspace/20pkt_traffic_model/target_model_weights'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select slot only base on distance\n",
    "def slot_scoring(obs):\n",
    "    #values = np.random.random_sample((32,)) / 20\n",
    "    \n",
    "    values = np.zeros((32,)) \n",
    "    #values[np.nonzero(obs[:n_slotUsedTable])] = -np.inf\n",
    "    \n",
    "    top_index = []\n",
    "    for i in range(3):\n",
    "        top_index.append(np.ravel(np.argwhere(obs[:n_slotUsedTable]==i+1)))\n",
    "    \n",
    "    for act in range(32):\n",
    "        r = 0\n",
    "        max_r = 0\n",
    "        for i in range(3):\n",
    "            if any(act < idx for idx in top_index[i]):\n",
    "                distance_map = top_index[i] - act\n",
    "                distance = min(np.where(distance_map>0,distance_map,np.inf))\n",
    "                closest_distance = 0\n",
    "                for idx in range(closest_distance):\n",
    "                    closest_distance = closest_distance + 1 if obs[act+idx] == 0 else closest_distance\n",
    "                    \n",
    "                distance_reward = (1.5-pow((closest_distance/10),0.2))\n",
    "                size_weight = _obs[-(3-i)]\n",
    "                max_r =  max((distance_reward*size_weight/3 + throughput/max_throughput/2),max_r) \n",
    "\n",
    "                        \n",
    "\n",
    "        values[act] = r + max_r\n",
    "    \n",
    "    values[np.nonzero(obs[:n_slotUsedTable])] = -np.inf\n",
    "    \n",
    "    #print (\"obs:\",obs[:n_slotUsedTable])\n",
    "    #print (\"values:\",values)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random select N slot \n",
    "def random_select(obs, max_slot):\n",
    "    nonusedSlot = np.where(obs==0)[0]\n",
    "    action = np.sort(np.random.choice(nonusedSlot,max_slot,replace=False))\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slot selection module\n",
    "def slot_select(q_values, free_slotNum, slot_table):\n",
    "    # get top N value of q value\n",
    "    q_values[np.nonzero(slot_table)] = -np.inf\n",
    "    action_unsort = np.argpartition(q_values,-MAXSLOTS)[-MAXSLOTS:]\n",
    "    \n",
    "    # sort action num by q value\n",
    "    action = action_unsort[np.argsort(-1*q_values[action_unsort])]\n",
    "    \n",
    "    \n",
    "    max_choosable_slotNum = min(free_slotNum, MAXSLOTS)\n",
    "    # Using the queuing bytes to determine how many data slot would be choose\n",
    "    action_num = 0 if queueBytes == 0 else min(max(0, int(queueBytes/6250 - 0.3)) + 1, max_choosable_slotNum)\n",
    "        \n",
    "    action[action_num:] = [-1] * (MAXSLOTS-action_num)\n",
    "    \n",
    "    # Sort action num\n",
    "    action[:action_num] = np.sort(action[:action_num])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got new port for ns3gm interface:  6470\n",
      "Start command:  /tf/ns3-gym/waf --run \"tdma-rl --openGymPort=6470 --simSeed=4282876139 pktNum=5\"\n",
      "Started ns3 simulation script, Process Id:  901\n"
     ]
    },
    {
     "ename": "Again",
     "evalue": "Resource temporarily unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAgain\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4659b7a20a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msimArgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pktNum'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpktNum\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#env = gym.make('ns3-v0') # issue of socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns3env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs3Env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimArgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimArgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#env.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/ns3gym/ns3env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stepTime, port, startSim, simSeed, simArgs, debug)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns3ZmqBridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNs3ZmqBridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartSim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimSeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns3ZmqBridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns3ZmqBridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns3ZmqBridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_observation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/ns3gym/ns3env.py\u001b[0m in \u001b[0;36minitialize_env\u001b[0;34m(self, stepInterval)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepInterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0msimInitMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimInitMsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0msimInitMsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAgain\u001b[0m: Resource temporarily unavailable"
     ]
    }
   ],
   "source": [
    "throughput_by_pktNum = []\n",
    "delay_by_pktNum = []\n",
    "for i in range (6):\n",
    "    optimized_choice = []\n",
    "    throughput_list = []\n",
    "    throughput_history = []\n",
    "    delay_list = []\n",
    "    delay_history = []\n",
    "    pktNum = (i+1)*5\n",
    "    max_throughput = 25*576*pktNum #  send_25times_per_sec*packetsize*packetnum\n",
    "    \n",
    "    simArgs = {'pktNum': pktNum}\n",
    "    #env = gym.make('ns3-v0') # issue of socket\n",
    "    env = ns3env.Ns3Env(simArgs=simArgs,debug=True)\n",
    "\n",
    "    #env.reset()\n",
    "    ob_space = env.observation_space\n",
    "    ac_space = env.action_space\n",
    "    ob_space_n = ob_space['slotUsedTable'].shape[0] + ob_space['pktBytes'].shape[0] - 1\n",
    "    n_slotUsedTable = ob_space['slotUsedTable'].shape[0]\n",
    "\n",
    "\n",
    "    print(\"Observation space: \", ob_space,  ob_space.dtype)\n",
    "    print(\"Action space: \", ac_space, ac_space.dtype)\n",
    "    print(\"n_action: \",ac_space.shape[0])\n",
    "    \n",
    "    # initiailize eval_net target_net\n",
    "    eval_model = Eval_Model(num_actions=ac_space.shape[0])\n",
    "    target_model = Target_Model(num_actions=ac_space.shape[0])\n",
    "    try:\n",
    "        # load weight from file\n",
    "        eval_model.load_weights(eval_model_weights_path)\n",
    "        target_model.load_weights(target_model_weights_path)\n",
    "        print('Load weights from ',eval_model_weights_path,\" and \",target_model_weights_path)\n",
    "    except:\n",
    "        print('Create new model')\n",
    "\n",
    "    # create DQN model\n",
    "    RL = DeepQNetwork(ac_space.shape[0], MAXSLOTS, ob_space_n,\n",
    "                      eval_model, target_model, learning_rate, reward_decay, e_greedy, \n",
    "                      replace_target_iter, memory_size, batch_size)\n",
    "    \n",
    "    for episode in range(training_episodes):\n",
    "        # For each 10 round, would have 1 round stop update model and select slot without 10% random select\n",
    "        isTraining = True if (episode+1)%10 != 0 else False\n",
    "        if isTraining:\n",
    "            print (\"-----------------------episodes: \", episode, \" (Training)----------------------\")\n",
    "        else:\n",
    "            print (\"-----------------------episodes: \", episode, \" (Testing)-----------------------\")\n",
    "\n",
    "        stepIdx = 0\n",
    "        total_choice_counter = 0\n",
    "        optimized_choice_counter = 0\n",
    "        nonLearn_data = 0\n",
    "        throughput = 0\n",
    "        delay = 0\n",
    "\n",
    "        # Initial environment\n",
    "        _obs = env.reset()\n",
    "        queueBytes = _obs[1][0]\n",
    "\n",
    "        free_slotNum = min(n_slotUsedTable - np.nonzero(_obs[0])[0].size,MAXSLOTS)\n",
    "\n",
    "        _obs = np.array(list(_obs[0]) + list(_obs[1][1:]))\n",
    "        _obs = np.pad(_obs,(0, ob_space_n - _obs.size), constant_values = 0)\n",
    "\n",
    "\n",
    "        while True:\n",
    "            stepIdx += 1\n",
    "\n",
    "            # Get Action\n",
    "            # Select slot by RL algorithm\n",
    "            action = np.array(slot_select(RL.choose_action(_obs, isTraining),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "\n",
    "            # Select slot by random select 2 slot\n",
    "            #action = np.array(random_select(_obs,2))\n",
    "            # Select slot by random select with queued information\n",
    "            #action = np.array(slot_select(np.random.random_sample(ac_space.shape[0]),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "            # Select slot by rule (distance reward)\n",
    "            #action = np.array(slot_select(slot_scoring(_obs),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "\n",
    "            #print(\"---action: \", action)\n",
    "            # Interact with environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "            if info == \"TimeOut\":\n",
    "                print(\"Step: \", stepIdx)\n",
    "\n",
    "                print (\"Throughput(Byte) :\", throughput)\n",
    "                print (\"delay(μs) :\", delay)\n",
    "\n",
    "                if total_choice_counter != 0:\n",
    "                    optimized_choice.append(optimized_choice_counter/total_choice_counter)\n",
    "\n",
    "                throughput_list.append(throughput)\n",
    "                delay_list.append(delay)\n",
    "\n",
    "                break\n",
    "\n",
    "            # Get free slot number\n",
    "            free_slotNum = min(n_slotUsedTable - np.nonzero(obs[0])[0].size,MAXSLOTS)\n",
    "\n",
    "            # Get queuing bytes\n",
    "            queueBytes = obs[1][0]\n",
    "\n",
    "            # Since there are multiple action in one step,\n",
    "            # according to each action, it would have one reward.\n",
    "            # But in ns3gym, the reward type is float, it means it can only return one reward,\n",
    "            # we use the return info to send multiple reward, throughput, delay\n",
    "\n",
    "            reward_all = [float(r) for r in info.split(',')]\n",
    "            throughput = reward_all[-2]\n",
    "            delay = reward_all[-1]\n",
    "            reward_all = reward_all[:-2]\n",
    "\n",
    "            if throughput != 0:\n",
    "                throughput_history.append(throughput)\n",
    "\n",
    "            if delay != 0:\n",
    "                delay_history.append(delay)\n",
    "\n",
    "            #reward_all = [0,0,0]\n",
    "            info = stepIdx\n",
    "\n",
    "            #print(\"Step: \", stepIdx)\n",
    "            #print(\"---obs, reward, done, info: \", obs, reward_all, done, info)\n",
    "\n",
    "            # Change data type\n",
    "            obs = np.array(list(obs[0]) + list(obs[1][1:]),dtype=float)\n",
    "            # padding 0 if K < 3 (Top1_queuedBytes,Top2_queuedBytes) -> (Top1_queuedBytes,Top2_queuedBytes,0)\n",
    "            obs = np.pad(obs,(0, ob_space_n - obs.size), constant_values = 0)\n",
    "            # Normalize queuebytes\n",
    "            obs[n_slotUsedTable:] = obs[n_slotUsedTable:]/queueBytes if queueBytes != 0 else np.zeros(3)\n",
    "\n",
    "            top_index = []\n",
    "            for i in range(3):\n",
    "                # Get the slot used by top K next hop\n",
    "                top_index.append(np.ravel(np.argwhere(_obs[:n_slotUsedTable]==i+1)))\n",
    "\n",
    "            # Calculate r_i of each a_i\n",
    "            for act, r in zip(action,reward_all):\n",
    "                if act == -1:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                total_choice_counter += 1\n",
    "                nonLearn_data += 1\n",
    "\n",
    "                # Only calculate reward of the non-collision action\n",
    "                if (act not in np.nonzero(_obs[:n_slotUsedTable])[0]):\n",
    "                    r += 0.2\n",
    "\n",
    "                    #print (\"act:\",act)\n",
    "                    #print (\"r:\",r)\n",
    "                    #print (\"throughput:\",throughput)\n",
    "\n",
    "                    isChosen = False\n",
    "                    max_r = 0\n",
    "                    # Add optimized reward\n",
    "                    for i in range(3):\n",
    "                        if any(act < idx for idx in top_index[i]):\n",
    "                            distance_map = top_index[i] - act\n",
    "                            distance = min(np.where(distance_map>0,distance_map,np.inf))\n",
    "                            closest_distance = 0\n",
    "                            for idx in range(closest_distance):\n",
    "                                closest_distance = closest_distance + 1 if obs[act+idx] == 0 else closest_distance\n",
    "\n",
    "                            distance_reward = (1.7-pow((closest_distance/10),0.2))\n",
    "                            size_weight = _obs[-(3-i)]\n",
    "                            max_r =  max((distance_reward*size_weight*0.9 + (throughput/max_throughput/2)*0.1),max_r) \n",
    "\n",
    "                            if not isChosen:\n",
    "                                isChosen = True\n",
    "                                optimized_choice_counter += 1\n",
    "\n",
    "                    if all(idx.size == 0 for idx in top_index):\n",
    "                        max_r = throughput/max_throughput/2\n",
    "\n",
    "\n",
    "\n",
    "                if isTraining:\n",
    "                    r += max_r\n",
    "                    RL.store_transition(_obs, act, r, obs)\n",
    "\n",
    "\n",
    "\n",
    "            #if stepIdx % 10000 == 0:\n",
    "            #    print(\"Step: \", stepIdx)\n",
    "\n",
    "            if (total_choice_counter > 5000) and (nonLearn_data > 10) and isTraining:\n",
    "                nonLearn_data = 0\n",
    "                RL.learn()\n",
    "\n",
    "            # swap observation\n",
    "            _obs = obs\n",
    "\n",
    "\n",
    "            if done:\n",
    "                print(\"Step: \", stepIdx)\n",
    "                print (\"done\")\n",
    "\n",
    "                print (\"Throughput(Byte) :\", throughput)\n",
    "                print (\"delay(μs) :\", delay)\n",
    "\n",
    "                if total_choice_counter != 0:\n",
    "                    optimized_choice.append(optimized_choice_counter/total_choice_counter)\n",
    "\n",
    "                throughput_list.append(throughput)\n",
    "                delay_list.append(delay)\n",
    "\n",
    "                break\n",
    "\n",
    "            #break\n",
    "    avg_throughput = sum(throughput_list)/len(throughput_list)\n",
    "    avg_delay = sum(delay_list)/len(delay_list)\n",
    "    print (\"Avg Throughput:{}\".format(avg_throughput))\n",
    "    print (\"Avg Delay:{}\".format(avg_delay))\n",
    "    throughput_by_pktNum.append(avg_throughput)\n",
    "    delay_by_pktNum.append(avg_delay)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # save weight to file\n",
    "    eval_model.save_weights('model_weight/{}pkt/eval_model_weights'.format(pktNum))\n",
    "    target_model.save_weights('model_weight/{}pkt/target_model_weights'.format(pktNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create ns3 environment\n",
    "# simArgs = {\n",
    "#     \"pktNum\":15\n",
    "# }\n",
    "# #env = gym.make('ns3-v0') # issue of socket\n",
    "# env = ns3env.Ns3Env(simArgs=simArgs,debug=True)\n",
    "\n",
    "# #env.reset()\n",
    "\n",
    "# ob_space = env.observation_space\n",
    "# ac_space = env.action_space\n",
    "# ob_space_n = ob_space['slotUsedTable'].shape[0] + ob_space['pktBytes'].shape[0] - 1\n",
    "# n_slotUsedTable = ob_space['slotUsedTable'].shape[0]\n",
    "\n",
    "\n",
    "# print(\"Observation space: \", ob_space,  ob_space.dtype)\n",
    "# print(\"Action space: \", ac_space, ac_space.dtype)\n",
    "# print(\"n_action: \",ac_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(training_episodes):\n",
    "#     # For each 10 round, would have 1 round stop update model and select slot without 10% random select\n",
    "#     isTraining = True if (episode+1)%10 != 0 else False\n",
    "#     if isTraining:\n",
    "#         print (\"-----------------------episodes: \", episode, \" (Training)----------------------\")\n",
    "#     else:\n",
    "#         print (\"-----------------------episodes: \", episode, \" (Testing)-----------------------\")\n",
    "\n",
    "#     stepIdx = 0\n",
    "#     total_choice_counter = 0\n",
    "#     optimized_choice_counter = 0\n",
    "#     nonLearn_data = 0\n",
    "#     throughput = 0\n",
    "#     delay = 0\n",
    "    \n",
    "#     # Initial environment\n",
    "#     _obs = env.reset()\n",
    "#     queueBytes = _obs[1][0]\n",
    "    \n",
    "#     free_slotNum = min(n_slotUsedTable - np.nonzero(_obs[0])[0].size,MAXSLOTS)\n",
    "    \n",
    "#     _obs = np.array(list(_obs[0]) + list(_obs[1][1:]))\n",
    "#     _obs = np.pad(_obs,(0, ob_space_n - _obs.size), constant_values = 0)\n",
    "    \n",
    "    \n",
    "#     while True:\n",
    "#         stepIdx += 1\n",
    "\n",
    "#         # Get Action\n",
    "#         # Select slot by RL algorithm\n",
    "#         action = np.array(slot_select(RL.choose_action(_obs, isTraining),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "        \n",
    "#         # Select slot by random select 2 slot\n",
    "#         #action = np.array(random_select(_obs,2))\n",
    "#         # Select slot by random select with queued information\n",
    "#         #action = np.array(slot_select(np.random.random_sample(ac_space.shape[0]),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "#         # Select slot by rule (distance reward)\n",
    "#         #action = np.array(slot_select(slot_scoring(_obs),free_slotNum,_obs[:n_slotUsedTable]))\n",
    "        \n",
    "#         #print(\"---action: \", action)\n",
    "#         # Interact with environment\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "\n",
    "        \n",
    "#         if info == \"TimeOut\":\n",
    "#             print(\"Step: \", stepIdx)\n",
    "            \n",
    "#             print (\"Throughput(Byte) :\", throughput)\n",
    "#             print (\"delay(μs) :\", delay)\n",
    "\n",
    "#             if total_choice_counter != 0:\n",
    "#                 optimized_choice.append(optimized_choice_counter/total_choice_counter)\n",
    "            \n",
    "#             throughput_list.append(throughput)\n",
    "#             delay_list.append(delay)\n",
    "            \n",
    "#             break\n",
    "        \n",
    "#         # Get free slot number\n",
    "#         free_slotNum = min(n_slotUsedTable - np.nonzero(obs[0])[0].size,MAXSLOTS)\n",
    "        \n",
    "#         # Get queuing bytes\n",
    "#         queueBytes = obs[1][0]\n",
    "        \n",
    "#         # Since there are multiple action in one step,\n",
    "#         # according to each action, it would have one reward.\n",
    "#         # But in ns3gym, the reward type is float, it means it can only return one reward,\n",
    "#         # we use the return info to send multiple reward, throughput, delay\n",
    "        \n",
    "#         reward_all = [float(r) for r in info.split(',')]\n",
    "#         throughput = reward_all[-2]\n",
    "#         delay = reward_all[-1]\n",
    "#         reward_all = reward_all[:-2]\n",
    "        \n",
    "#         if throughput != 0:\n",
    "#             throughput_history.append(throughput)\n",
    "        \n",
    "#         if delay != 0:\n",
    "#             delay_history.append(delay)\n",
    "            \n",
    "#         #reward_all = [0,0,0]\n",
    "#         info = stepIdx\n",
    "        \n",
    "#         #print(\"Step: \", stepIdx)\n",
    "#         #print(\"---obs, reward, done, info: \", obs, reward_all, done, info)\n",
    "        \n",
    "#         # Change data type\n",
    "#         obs = np.array(list(obs[0]) + list(obs[1][1:]),dtype=float)\n",
    "#         # padding 0 if K < 3 (Top1_queuedBytes,Top2_queuedBytes) -> (Top1_queuedBytes,Top2_queuedBytes,0)\n",
    "#         obs = np.pad(obs,(0, ob_space_n - obs.size), constant_values = 0)\n",
    "#         # Normalize queuebytes\n",
    "#         obs[n_slotUsedTable:] = obs[n_slotUsedTable:]/queueBytes if queueBytes != 0 else np.zeros(3)\n",
    "        \n",
    "#         top_index = []\n",
    "#         for i in range(3):\n",
    "#             # Get the slot used by top K next hop\n",
    "#             top_index.append(np.ravel(np.argwhere(_obs[:n_slotUsedTable]==i+1)))\n",
    "        \n",
    "#         # Calculate r_i of each a_i\n",
    "#         for act, r in zip(action,reward_all):\n",
    "#             if act == -1:\n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "#             total_choice_counter += 1\n",
    "#             nonLearn_data += 1\n",
    "            \n",
    "#             # Only calculate reward of the non-collision action\n",
    "#             if (act not in np.nonzero(_obs[:n_slotUsedTable])[0]):\n",
    "#                 r += 0.2\n",
    "                \n",
    "#                 #print (\"act:\",act)\n",
    "#                 #print (\"r:\",r)\n",
    "#                 #print (\"throughput:\",throughput)\n",
    "                \n",
    "#                 isChosen = False\n",
    "#                 max_r = 0\n",
    "#                 # Add optimized reward\n",
    "#                 for i in range(3):\n",
    "#                     if any(act < idx for idx in top_index[i]):\n",
    "#                         distance_map = top_index[i] - act\n",
    "#                         distance = min(np.where(distance_map>0,distance_map,np.inf))\n",
    "#                         closest_distance = 0\n",
    "#                         for idx in range(closest_distance):\n",
    "#                             closest_distance = closest_distance + 1 if obs[act+idx] == 0 else closest_distance\n",
    "                    \n",
    "#                         distance_reward = (1.7-pow((closest_distance/10),0.2))\n",
    "#                         size_weight = _obs[-(3-i)]\n",
    "#                         max_r =  max((distance_reward*size_weight*0.9 + (throughput/max_throughput/2)*0.1),max_r) \n",
    "                        \n",
    "#                         if not isChosen:\n",
    "#                             isChosen = True\n",
    "#                             optimized_choice_counter += 1\n",
    "                            \n",
    "#                 if all(idx.size == 0 for idx in top_index):\n",
    "#                     max_r = throughput/max_throughput/2\n",
    "                \n",
    "                \n",
    "                \n",
    "#             if isTraining:\n",
    "#                 r += max_r\n",
    "#                 RL.store_transition(_obs, act, r, obs)\n",
    "            \n",
    "\n",
    "                \n",
    "#         #if stepIdx % 10000 == 0:\n",
    "#         #    print(\"Step: \", stepIdx)\n",
    "        \n",
    "#         if (total_choice_counter > 5000) and (nonLearn_data > 10) and isTraining:\n",
    "#             nonLearn_data = 0\n",
    "#             RL.learn()\n",
    "            \n",
    "#         # swap observation\n",
    "#         _obs = obs\n",
    "        \n",
    "        \n",
    "#         if done:\n",
    "#             print(\"Step: \", stepIdx)\n",
    "#             print (\"done\")\n",
    "            \n",
    "#             print (\"Throughput(Byte) :\", throughput)\n",
    "#             print (\"delay(μs) :\", delay)\n",
    "            \n",
    "#             if total_choice_counter != 0:\n",
    "#                 optimized_choice.append(optimized_choice_counter/total_choice_counter)\n",
    "            \n",
    "#             throughput_list.append(throughput)\n",
    "#             delay_list.append(delay)\n",
    "            \n",
    "#             break\n",
    "            \n",
    "#         #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step counter of each episode\n",
    "# plt.plot(optimized_choice)\n",
    "# plt.xlabel('Episodes') \n",
    "# plt.ylabel('') \n",
    "# plt.title(\"Optimized Choice\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(throughput_history)\n",
    "# plt.xlabel('Episodes') \n",
    "# plt.ylabel('Bytes') \n",
    "# plt.title(\"Throughput History\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(throughput_list)\n",
    "# plt.xlabel('Episodes') \n",
    "# plt.ylabel('Bytes') \n",
    "# plt.title(\"Throughput\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(delay_history)\n",
    "# plt.xlabel('Episodes') \n",
    "# plt.ylabel('us') \n",
    "# plt.title(\"Delay History\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(delay_list)\n",
    "# plt.xlabel('Episodes') \n",
    "# plt.ylabel('us') \n",
    "# plt.title(\"Delay\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Avg Throughput:{}\".format(sum(throughput_list)/len(throughput_list)))\n",
    "# print (\"Avg Delay:{}\".format(sum(delay_list)/len(delay_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save weight to file\n",
    "# eval_model.save_weights('/workspace/80disweight_20globalthrought_model/eval_model_weights')\n",
    "# target_model.save_weights('/workspace/80disweight_20globalthrought_model/target_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
